{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Homework - Mission to Mars\n",
    "----\n",
    "### Web Scraping Homework (web-scraping-challenge)    |    by: Shane Gatenby\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chrome Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mac Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/chromedriver\r\n"
     ]
    }
   ],
   "source": [
    "# Identify chromedriver & location\n",
    "# https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create executable_path and open chrome browser window\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PC/Windows Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "# browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NASA Mars News\n",
    "* Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "mars_news_url = 'https://mars.nasa.gov/news/'\n",
    "browser.visit(mars_news_url)\n",
    "\n",
    "# Wait 3 seconds before proceeding to give browser time to fully load\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object & parse with 'html.parser'\n",
    "mars_news_html_text = browser.html\n",
    "mars_news_soup = BeautifulSoup(mars_news_html_text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print formatted verion of BeautifulSoup object to examine\n",
    "# print(mars_news_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mars News Headline Title\n",
    "# news_hl_title = mars_news_soup.find('div', class_='list_text').find('div', class_='content_title').text\n",
    "news_hl_title = mars_news_soup.find_all('div', class_='list_text')[0].find('div', class_='content_title').text\n",
    "\n",
    "# Mars News Headline Teaser Paragraph\n",
    "# news_teaser_p = mars_news_soup.find('div', class_='list_text').find('div', class_='article_teaser_body').text\n",
    "news_teaser_p = mars_news_soup.find_all('div', class_='list_text')[0].find('div', class_='article_teaser_body').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virginia Middle School Student Earns Honor of Naming NASA's Next Mars Rover\n",
      "----------\n",
      "NASA chose a seventh-grader from Virginia as winner of the agency's \"Name the Rover\" essay contest. Alexander Mather's entry for \"Perseverance\" was voted tops among 28,000 entries. \n"
     ]
    }
   ],
   "source": [
    "# Visualize\n",
    "print(news_hl_title)\n",
    "print('----------')\n",
    "print(news_teaser_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JPL Mars Space Images - Featured Image\n",
    "\n",
    "* Visit the url for JPL Featured Space Image here: https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars.\n",
    "* Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\n",
    "* Make sure to find the image url to the full size .jpg image.\n",
    "* Make sure to save a complete url string for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "jpl_base_url = 'https://www.jpl.nasa.gov'\n",
    "jpl_image_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(jpl_image_url)\n",
    "\n",
    "# Wait 3 seconds before proceeding to give browser time to fully load\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object & parse with 'html.parser'\n",
    "jpl_image_html_text = browser.html\n",
    "jpl_image_soup = BeautifulSoup(jpl_image_html_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print formatted verion of BeautifulSoup object to examine\n",
    "# print(jpl_image_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"background-image: url('/spaceimages/images/wallpaper/PIA15254-1920x1200.jpg');\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find featured image url path within the article tag\n",
    "jpl_image_soup.find('article', class_=\"carousel_item\")['style']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spaceimages/images/wallpaper/PIA15254-1920x1200.jpg\n"
     ]
    }
   ],
   "source": [
    "# Use split() to extract the portion of the url needed, setting to variable for later use\n",
    "jpl_image_extra_path = jpl_image_soup.find('article', class_=\"carousel_item\")['style'].\\\n",
    "                        split(\"url('\")[1].\\\n",
    "                        split(\"');\")[0]\n",
    "print(jpl_image_extra_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'background-image: url(\"/spaceimages/images/wallpaper/PIA15254-1920x1200.jpg\");'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way: using Splinter and browser.find_by_xpath:\n",
    "\n",
    "# Set jpl_featured_image_xpath to XPath copied from Chrome inspector tool\n",
    "jpl_featured_image_xpath = '//*[@id=\"page\"]/section[1]/div/div/article'\n",
    "\n",
    "# Pass variable through to spliter browser.find_by_xpath method, traverse to ['style']\n",
    "browser.find_by_xpath(jpl_featured_image_xpath)['style']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/spaceimages/images/wallpaper/PIA15254-1920x1200.jpg\n"
     ]
    }
   ],
   "source": [
    "# Use split to extract the portion of the url needed, setting to variable for later use\n",
    "jpl_image_extra_path2 = browser.find_by_xpath(jpl_featured_image_xpath)['style'].\\\n",
    "                        split('url(\"')[1].\\\n",
    "                        split('\");')[0]\n",
    "print(jpl_image_extra_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.jpl.nasa.gov/spaceimages/images/wallpaper/PIA15254-1920x1200.jpg\n",
      "https://www.jpl.nasa.gov/spaceimages/images/wallpaper/PIA15254-1920x1200.jpg\n"
     ]
    }
   ],
   "source": [
    "# Set image path to be a concat of jpl_base_url + jpl_image_extra_path\n",
    "featured_image_url = f'{jpl_base_url}{jpl_image_extra_path}'\n",
    "featured_image_url2 = f'{jpl_base_url}{jpl_image_extra_path2}'\n",
    "print(featured_image_url)\n",
    "print(featured_image_url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dusty Space Cloud\n"
     ]
    }
   ],
   "source": [
    "# Find featured image title within the article tag setting to variable for later use\n",
    "featured_image_title = jpl_image_soup.find('article', class_=\"carousel_item\")['alt']\n",
    "print(featured_image_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mars Weather\n",
    "\n",
    "* Visit the Mars Weather twitter account here (https://twitter.com/marswxreport?lang=en)and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called mars_weather.\n",
    "* Note: Be sure you are not signed in to twitter, or scraping may become more difficult.\n",
    "* Note: Twitter frequently changes how information is presented on their website. If you are having difficulty getting the correct html tag data, consider researching Regular Expression Patterns and how they can be used in combination with the .find() method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "mars_wx_twitter_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser.visit(mars_wx_twitter_url)\n",
    "\n",
    "# Wait 3 seconds before proceeding to give browser time to fully load\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object & parse with 'html.parser'\n",
    "mars_wx_twitter_html_text = browser.html\n",
    "mars_wx_twitter_soup = BeautifulSoup(mars_wx_twitter_html_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print formatted verion of BeautifulSoup object to examine\n",
    "# print(mars_wx_twitter_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InSight sol 453 (2020-03-05) low -95.1ºC (-139.1ºF) high -10.8ºC (12.6ºF)\n",
      "winds from the SSW at 6.0 m/s (13.3 mph) gusting to 21.4 m/s (47.9 mph)\n",
      "pressure at 6.30 hPa\n"
     ]
    }
   ],
   "source": [
    "# Using Splinter and browser.find_by_xpath:\n",
    "\n",
    "# Set latest_mars_wx_tweet_xpath to XPath copied from Chrome inspector tool\n",
    "latest_mars_wx_tweet_xpath = '//*[@id=\"react-root\"]/div/div/div/main/div/div/div/div[1]/div/div/div/div/div[2]/section/div/div/div/div[1]/div/article/div/div[2]/div[2]/div[2]/span'\n",
    "\n",
    "# Pass variable through to spliter browser.find_by_xpath method\n",
    "latest_mars_wx_tweet = browser.find_by_xpath(latest_mars_wx_tweet_xpath).text\n",
    "print(latest_mars_wx_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mars_wx_tweets = mars_wx_twitter_soup.find_all('div', class_=\"css-901oao r-hkyrab r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")\n",
    "# mars_wx_tweets2 = mars_wx_twitter_soup2.find_all('div', class_=\"js-tweet-text-container\")\n",
    "\n",
    "# print(len(mars_wx_tweets))\n",
    "# print(len(mars_wx_tweets2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using browser.visit()\n",
    "mars_wx_tweets = mars_wx_twitter_soup.find_all('div', class_=\"css-901oao r-hkyrab r-1qd0xha r-a023e6 r-16dba41 r-ad9z0x r-bcqeeo r-bnwqim r-qvutc0\")\n",
    "\n",
    "# Iterate through Mars Weather tweets one by one, searching for <span> holding tweet text\n",
    "for mars_wx_tweet in mars_wx_tweets:\n",
    "    \n",
    "    # Set variable to tweet text\n",
    "    mars_wx_wx_tweet = mars_wx_tweet.find('span', class_='css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0').text\n",
    "    \n",
    "    # Remove unwanted charaters (pic link, if they exist at end of tweet), use split() to pull them off\n",
    "    mars_wx_wx_tweet = mars_wx_wx_tweet.split('pic.twitter.com/')[0]\n",
    "    mars_wx_wx_tweet = mars_wx_wx_tweet.replace('\\n',' ')\n",
    "    print(mars_wx_wx_tweet)\n",
    "    \n",
    "    # Determine if tweet is weather related and exit, else pass and move on to next tweet\n",
    "    if 'InSight sol ' in mars_wx_wx_tweet:\n",
    "        break\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mars_wx_wx_tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5d61890e3a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set first weather related tweet to required variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmars_weather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmars_wx_wx_tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmars_weather\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mars_wx_wx_tweet' is not defined"
     ]
    }
   ],
   "source": [
    "# Set first weather related tweet to required variable\n",
    "mars_weather = mars_wx_wx_tweet\n",
    "print(mars_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InSight sol 453 (2020-03-05) low -95.1ºC (-139.1ºF) high -10.8ºC (12.6ºF) winds from the SSW at 6.0 m/s (13.3 mph) gusting to 21.4 m/s (47.9 mph) pressure at 6.30 hPa\n"
     ]
    }
   ],
   "source": [
    "# Using requests.get \n",
    "mars_wx_response2 = requests.get(mars_wx_twitter_url)\n",
    "mars_wx_twitter_soup2 = BeautifulSoup(mars_wx_response2.text, 'html.parser')\n",
    "\n",
    "# Print formatted verion of BeautifulSoup object to examine\n",
    "# print(mars_wx_twitter_soup2.prettify())\n",
    "\n",
    "mars_wx_tweets2 = mars_wx_twitter_soup2.find_all('div', class_=\"js-tweet-text-container\")\n",
    "\n",
    "# Iterate through Mars Weather tweets one by one, searching for <p> holding tweet text\n",
    "for mars_wx_tweet2 in mars_wx_tweets2:\n",
    "    \n",
    "    # Set variable to tweet text\n",
    "    mars_wx_wx_tweet2 = mars_wx_tweet2.find('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text').text\n",
    "    \n",
    "    # Remove unwanted charaters (pic link, if they exist at end of tweet), use split() to pull them off\n",
    "    mars_wx_wx_tweet2 = mars_wx_wx_tweet2.split('pic.twitter.com/')[0]\n",
    "    mars_wx_wx_tweet2 = mars_wx_wx_tweet2.replace('\\n',' ')\n",
    "    print(mars_wx_wx_tweet2)\n",
    "    \n",
    "    # Determine if tweet is weather related and exit, else pass and move on to next tweet\n",
    "    if 'InSight sol ' in mars_wx_wx_tweet2:\n",
    "        break\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set first weather related tweet to required variable\n",
    "# mars_weather = mars_wx_wx_tweet2\n",
    "# print(mars_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mars Facts\n",
    "\n",
    "* Visit the Mars Facts webpage here (https://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "* Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "mars_facts_url = 'https://space-facts.com/mars/'\n",
    "browser.visit(mars_facts_url)\n",
    "\n",
    "# Wait 3 seconds before proceeding to give browser time to fully load\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object & parse with 'html.parser'\n",
    "mars_facts_html_text = browser.html\n",
    "mars_facts_soup = BeautifulSoup(mars_facts_html_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print formatted verion of BeautifulSoup object to examine\n",
    "# print(mars_facts_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate and set variable for the mars_facts_table\n",
    "# mars_facts_table = mars_facts_soup.find('table', id='tablepress-p-mars-no-2').find('tbody')\n",
    "# print(mars_facts_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Splinter and browser.find_by_xpath:\n",
    "\n",
    "# Set mars_facts_table_xpath to XPath copied from Chrome inspector tool\n",
    "mars_facts_table_xpath = '//*[@id=\"tablepress-p-mars-no-2\"]'\n",
    "# mars_facts_table_xpath = '//*[@id=\"tablepress-p-mars-no-2\"]/tbody'\n",
    "\n",
    "# Pass variable through to spliter browser.find_by_xpath method\n",
    "mars_facts_table_html = browser.find_by_xpath(mars_facts_table_xpath).html\n",
    "\n",
    "# Add <table> tag around mars_facts_table_html to use in pd.read_html()\n",
    "mars_facts_table_html = f'<table> {mars_facts_table_html} </table>'\n",
    "# print(mars_facts_table_html)\n",
    "# mars_facts_table_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Equatorial Diameter:</td>\n",
       "      <td>6,792 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Polar Diameter:</td>\n",
       "      <td>6,752 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Mass:</td>\n",
       "      <td>6.39 × 10^23 kg (0.11 Earths)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Moons:</td>\n",
       "      <td>2 (Phobos &amp; Deimos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Orbit Distance:</td>\n",
       "      <td>227,943,824 km (1.38 AU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Orbit Period:</td>\n",
       "      <td>687 days (1.9 years)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Surface Temperature:</td>\n",
       "      <td>-87 to -5 °C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>First Record:</td>\n",
       "      <td>2nd millennium BC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Recorded By:</td>\n",
       "      <td>Egyptian astronomers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Value\n",
       "                                                   \n",
       "Equatorial Diameter:                       6,792 km\n",
       "Polar Diameter:                            6,752 km\n",
       "Mass:                 6.39 × 10^23 kg (0.11 Earths)\n",
       "Moons:                          2 (Phobos & Deimos)\n",
       "Orbit Distance:            227,943,824 km (1.38 AU)\n",
       "Orbit Period:                  687 days (1.9 years)\n",
       "Surface Temperature:                   -87 to -5 °C\n",
       "First Record:                     2nd millennium BC\n",
       "Recorded By:                   Egyptian astronomers"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the mars_facts_table_html html table into a pandas DataFrame\n",
    "mars_facts_df = pd.read_html(mars_facts_table_html)[0]\n",
    "\n",
    "# Set first column as the DataFrame index, removed column header name and rename 2nd column\n",
    "mars_facts_df = mars_facts_df.set_index(0)\n",
    "mars_facts_df = mars_facts_df.rename_axis('')\n",
    "mars_facts_df = mars_facts_df.rename(columns={1:'Value'})\n",
    "\n",
    "# Visualize the DataFrame\n",
    "mars_facts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to_html method to create html table from mars_facts_df DataFrame\n",
    "mars_facts_html_table = mars_facts_df.to_html()\n",
    "\n",
    "# Remove unwanted charaters ('\\n')\n",
    "# mars_facts_html_table = mars_facts_html_table.replace('\\n','')\n",
    "# print(mars_facts_html_table)\n",
    "# mars_facts_html_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mars Hemispheres\n",
    "\n",
    "* Visit the USGS Astrogeology site here (https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres.\n",
    "* You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "* Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys img_url and title.\n",
    "* Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "mars_hemispheres_base_url = 'https://astrogeology.usgs.gov'\n",
    "mars_hemispheres_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(mars_hemispheres_url)\n",
    "\n",
    "# Wait 3 seconds before proceeding to give browser time to fully load\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BeautifulSoup object & parse with 'html.parser'\n",
    "mars_hemispheres_html_text = browser.html\n",
    "mars_hemispheres_soup = BeautifulSoup(mars_hemispheres_html_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print formatted verion of BeautifulSoup object to examine\n",
    "# print(mars_hemispheres_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to hold all of Mars Hemisphere key: value pairs for title and img_url\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# Iterate through the mars_hemispheres_soup html to find needed elements\n",
    "for hemisphere in mars_hemispheres_soup.find_all('div', class_='item'):\n",
    "    # Locate and set variable for Hemisphere Name/title and use .split() to remove ' Enhanced'\n",
    "    # print(hemisphere.find('div', class_='description').find('h3').text.split(' Enhanced')[0])\n",
    "    title = hemisphere.find('div', class_='description').find('h3').text.split(' Enhanced')[0]\n",
    "    \n",
    "    # Locate and set variable for Hemisphere's individual url (concat to mars_hemispheres_base_url)\n",
    "    # print(hemisphere.find('div', class_='description').find('a')['href'])\n",
    "    hemisphere_url_extra_path = hemisphere.find('div', class_='description').find('a')['href']\n",
    "    hemisphere_url = f'{mars_hemispheres_base_url}{hemisphere_url_extra_path}'\n",
    "    # print(hemisphere_url)\n",
    "    \n",
    "    # Navigate spliter's browser to each hemisphere's url to scrape that hemisphere's full resolution image url\n",
    "    browser.visit(hemisphere_url)\n",
    "\n",
    "    # Wait 3 seconds before proceeding to give browser time to fully load\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Create BeautifulSoup object & parse with 'html.parser'\n",
    "    hemisphere_html_text = browser.html\n",
    "    hemisphere_soup = BeautifulSoup(hemisphere_html_text, 'html.parser')\n",
    "    \n",
    "    # Locate and set variable for each hemisphere's individual full resolution image url\n",
    "    # print(hemisphere_soup.find('div', class_='downloads').find('li').find('a')['href'])\n",
    "    hemisphere_img_url = hemisphere_soup.find('div', class_='downloads').find('li').find('a')['href']\n",
    "    \n",
    "    # Create a dictionary holding the Hemisphere's Name/title & it's full resolution image url\n",
    "    hemisphere_image_dict = {'title': title, 'img_url': hemisphere_img_url}\n",
    "    # print(hemisphere_image_dict)\n",
    "    \n",
    "    # Append hemisphere_image_urls dictionary to include Name/title and full resolution image url\n",
    "    hemisphere_image_urls.append(hemisphere_image_dict)\n",
    "    # print(hemisphere_image_urls)\n",
    "    # print(f'hemisphere_image_urls appended with: {hemisphere_image_dict}')\n",
    "    # print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere',\n",
       "  'img_url': 'http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the hemisphere_image_urls list of dictionaries\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser window opened by splinter\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - MongoDB and Flask Application\n",
    "----\n",
    "#### Use MongoDB with Flask templating to create a new HTML page that displays all of the information that was scraped from the URLs above.\n",
    "----\n",
    "* Start by converting your Jupyter notebook into a Python script called scrape_mars.py with a function called scrape that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data.\n",
    "* Next, create a route called /scrape that will import your scrape_mars.py script and call your scrape function.\n",
    "* Store the return value in Mongo as a Python dictionary.\n",
    "* Create a root route / that will query your Mongo database and pass the mars data into an HTML template to display the data.\n",
    "* Create a template HTML file called index.html that will take the mars data dictionary and display all of the data in the appropriate HTML elements. Use the following as a guide for what the final product should look like, but feel free to create your own design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script mission_to_mars_v1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Please see scrape_mars.py for script from this Jupyter Notebook that has been converted into a Python script for use by the app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv9b3cbb0139024dd3b43c9af8ac29f73c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
